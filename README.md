# llm-advers-eval
This repository supports “Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities” (Zhang et al., 2025). It provides code and data for evaluating how LLMs adapt under adversarial, game-theoretic, and cognitive-psychology-based tasks—revealing model vulnerabilities, adaptability, and fairness in decision-making.
